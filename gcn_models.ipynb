{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Utils\n",
    "from tqdm.notebook import tqdm\n",
    "from ml_utils import *\n",
    "from machine_learning_models import *\n",
    "from fingerprints import *\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import random\n",
    "#deepchem\n",
    "import deepchem as dc\n",
    "from deepchem.models import GraphConvModel\n",
    "from IPython.core.display_functions import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models Parameters\n",
    "### Select the desired parameters to be used by GCN models\n",
    "<p>\n",
    "<li> <b>model_list</b>: ML/DL models for regression (GCN: Graph Neural Networks)</li>\n",
    "</p>\n",
    "<p>\n",
    "<li> <b>cv_fold</b>: Number do data splits (trials) to be performed</li>\n",
    "</p>\n",
    "<p>\n",
    "<li> <b>data_order</b>: Different data orders ('regular': Normal potency (y) order, 'y_rand': Randomized potency values) </li>\n",
    "</p>\n",
    "<p>\n",
    "<li> <b>compound_sets</b>: Compound sets to be generated ('Complete set': 100% compounds, 'Random set': Random set of compounds, 'Diverse set': Chemical diverse set of compounds) </li>\n",
    "</p>\n",
    "<p>\n",
    "<li> <b>compound_sets_size</b>: Compound sets size to be generated for 'Random' and 'Diverse' based on the size of the respective 'Complete' ('Complete set': 100% compounds, 'Random set': 25%, 'Diverse set': 25%) </li>\n",
    "</p>\n",
    "<p>\n",
    "<li> <b>params_dict</b>: GCN hyperparameter grid (nb_epoch: number of epochs, learning_rate, graph_conv_layer, dense_layer_size, dropout, number of atom features) </li>\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_list = ['GCN']\n",
    "cv_folds=10\n",
    "data_order = ['regular', 'y_rand']\n",
    "compound_sets = ['Complete set', 'Random set', 'Diverse set']\n",
    "compound_sets_size = 0.25\n",
    "\n",
    "params_dict = {\n",
    "     \"nb_epoch\":[100, 200],\n",
    "     \"learning_rate\":[0.01, 0.001],\n",
    "     \"n_tasks\":[1],\n",
    "     \"graph_conv_layers\":[[64, 64], [256, 256], [512, 512], [1024, 1024]],\n",
    "     \"dense_layer_size\":[64, 256, 512, 1024],\n",
    "     \"dropout\":[0.0],\n",
    "     \"mode\":[\"regression\"],\n",
    "     \"number_atom_features\":[75]}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data\n",
    "### Load compound database to be used for the regression models\n",
    "\n",
    "<li> <b>db_path</b>: dataset full path</li>\n",
    "</p>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Load actives dB\n",
    "db_path = './dataset/'\n",
    "# Load actives dB\n",
    "regression_db = pd.read_csv(os.path.join(db_path, f'chembl_30_IC50_10_tids_1000_CPDs.csv'))\n",
    "# Target Classes\n",
    "regression_tids = regression_db.chembl_tid.unique()[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GCN Models\n",
    "### Folowing code generates potency prediction based on GCN models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create saving path\n",
    "create_directory('./regression_results/')\n",
    "\n",
    "for data_ord in data_order:\n",
    "\n",
    "    performance_train_df = pd.DataFrame()\n",
    "    performance_test_df = pd.DataFrame()\n",
    "    predictions_test_df = pd.DataFrame()\n",
    "    parameter_resume = []\n",
    "\n",
    "    for target in tqdm(regression_tids):\n",
    "\n",
    "        # Select Target Database\n",
    "        regression_db_tid = regression_db.loc[regression_db.chembl_tid == target]\n",
    "\n",
    "        #compound potency\n",
    "        potency = regression_db_tid.pPot.values.tolist()\n",
    "\n",
    "        # Randomized Class potency\n",
    "        if data_ord == 'y_rand':\n",
    "            random.shuffle(potency)\n",
    "\n",
    "        for approach in compound_sets:\n",
    "            for i in range(3):\n",
    "                print(f'Training on {target} - {approach} - {data_ord}')\n",
    "\n",
    "                # Generate Mol object from SMILES\n",
    "                mols = [Chem.MolFromSmiles(smi) for smi in regression_db_tid.nonstereo_aromatic_smiles.tolist()]\n",
    "\n",
    "                # Data featurization\n",
    "                featurizer = dc.feat.ConvMolFeaturizer()\n",
    "                mol_graphs = featurizer.featurize(mols)\n",
    "\n",
    "                # Constructing Dataset\n",
    "                dataset = dc.data.NumpyDataset(X=mol_graphs, y=np.array(potency), ids=np.array(regression_db_tid.chembl_tid.values))\n",
    "\n",
    "                # Data Sampling Approaches\n",
    "                if approach == 'Random set':\n",
    "                    random.seed(i+1)\n",
    "                    mol_idx = random.sample([idx for idx in range(len(dataset))], int(compound_sets_size*len(dataset)))\n",
    "                    dataset = dataset.select(mol_idx)\n",
    "\n",
    "                elif approach == 'Diverse set':\n",
    "                    fp_bit_vec = ECFP4(regression_db_tid.nonstereo_aromatic_smiles.tolist())\n",
    "                    mol_idx = maxminpicker(fp_bit_vec, compound_sets_size, seed=i+1)\n",
    "                    dataset = dataset.select(mol_idx)\n",
    "\n",
    "                # Split dataset into TR and TE\n",
    "                data_splitter = ShuffleSplit(n_splits=cv_folds, random_state=20021997, test_size=0.2)\n",
    "                for trial, (train_idx, test_idx) in enumerate(data_splitter.split(dataset.X)):\n",
    "\n",
    "                    #Defining Training and Test sets\n",
    "                    training_set = dataset.select(train_idx)\n",
    "                    test_set = dataset.select(test_idx)\n",
    "\n",
    "                    # Initialize transformers for train_dataset\n",
    "                    transformers_train = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset)\n",
    "                    training_set = transformers_train.transform(training_set)\n",
    "\n",
    "                    # Initialize transformers for test_dataset\n",
    "                    transformers_test = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset)\n",
    "                    test_set = transformers_test.transform(test_set)\n",
    "\n",
    "                    # Split dataset into TR and internal Validation\n",
    "                    splitter = dc.splits.RandomSplitter()\n",
    "                    train_set, valid_set = splitter.train_test_split(training_set, seed=trial)\n",
    "\n",
    "                    for model in model_list:\n",
    "\n",
    "                        #Define random seed (reproducibility)\n",
    "                        tf.keras.utils.set_random_seed(trial)\n",
    "\n",
    "                        #Initialize GridSearch optimizer\n",
    "                        optimizer = dc.hyper.GridHyperparamOpt(dc.models.GraphConvModel)\n",
    "\n",
    "                        # Select optimization metric (MAE)\n",
    "                        metric = dc.metrics.Metric(dc.metrics.mae_score)\n",
    "\n",
    "                        # Best GCN model, parameters and final results\n",
    "                        best_model, best_params, all_results = optimizer.hyperparam_search(params_dict=params_dict,\n",
    "                                                                                           train_dataset=train_set,\n",
    "                                                                                           valid_dataset=valid_set,\n",
    "                                                                                           metric=metric,\n",
    "                                                                                           use_max=False,\n",
    "                                                                                           output_transformers=[transformers_train],\n",
    "                                                                                           #logdir=r'C:\\\\GCN\\\\'\n",
    "                                                                                           )\n",
    "\n",
    "                        # Define final GCN model\n",
    "                        def final_gcn(data, best_params):\n",
    "\n",
    "                            gcn = GraphConvModel(n_tasks=best_params[\"n_tasks\"],\n",
    "                                               graph_conv_layers=best_params[\"graph_conv_layers\"],\n",
    "                                               dropout=best_params[\"dropout\"],\n",
    "                                                mode=best_params[\"mode\"],\n",
    "                                               predictor_hidden_feats=best_params[\"dense_layer_size\"],\n",
    "                                               learning_rate=best_params[\"learning_rate\"],\n",
    "                                                )\n",
    "\n",
    "                            gcn.fit(data, nb_epoch=best_params[\"nb_epoch\"])\n",
    "\n",
    "                            return gcn\n",
    "\n",
    "                        #Best GCN model parameters\n",
    "                        opt_parameters_dict = {'model': model,\n",
    "                                               'trial': trial,\n",
    "                                               'Target ID': target,\n",
    "                                               'Approach':approach}\n",
    "\n",
    "                        for param, value in best_params.items():\n",
    "                            opt_parameters_dict[param] = value\n",
    "                        parameter_resume.append(opt_parameters_dict)\n",
    "\n",
    "                        # Generate final Model\n",
    "                        ml_model = final_gcn(training_set, best_params)\n",
    "\n",
    "                        # evaluate the model\n",
    "                        train_score = ml_model.evaluate(training_set, [metric], [transformers_train])\n",
    "                        test_score = ml_model.evaluate(test_set, [metric], [transformers_train])\n",
    "\n",
    "                        #TRAIN\n",
    "                        #Model Evaluation\n",
    "                        model_eval_train = Model_Evaluation(ml_model, training_set, transformers_train, model_id=model)\n",
    "\n",
    "                        #Performance df\n",
    "                        performance_train = model_eval_train.pred_performance\n",
    "                        performance_train[\"trial\"] = trial\n",
    "                        performance_train[\"Approach\"] = approach\n",
    "                        performance_train[\"Approach_trial\"] = i\n",
    "                        performance_train[\"data_order\"] = data_ord\n",
    "                        performance_train_df = pd.concat([performance_train_df, performance_train])\n",
    "\n",
    "                        #Model Evaluation\n",
    "                        model_eval_test = Model_Evaluation(ml_model, test_set, transformers_train, model_id=model)\n",
    "\n",
    "                        #Performance df\n",
    "                        performance_test = model_eval_test.pred_performance\n",
    "                        performance_test[\"trial\"] = trial\n",
    "                        performance_test[\"Approach\"] = approach\n",
    "                        performance_test[\"Approach_trial\"] = i\n",
    "                        performance_test[\"data_order\"] = data_ord\n",
    "                        performance_test_df = pd.concat([performance_test_df, performance_test])\n",
    "\n",
    "                        # Prediction df\n",
    "                        predictions_test = model_eval_test.predictions\n",
    "                        predictions_test[\"trial\"] = trial\n",
    "                        predictions_test[\"Approach\"] = approach\n",
    "                        predictions_test[\"Approach_trial\"] = i\n",
    "                        predictions_test[\"data_order\"] = data_ord\n",
    "                        predictions_test_df = pd.concat([predictions_test_df, predictions_test])\n",
    "\n",
    "                        del best_model, best_params, all_results, ml_model\n",
    "\n",
    "                if approach == 'Complete set':\n",
    "                    break\n",
    "\n",
    "    display(performance_test_df)\n",
    "    parameter_df = pd.DataFrame(parameter_resume)\n",
    "\n",
    "    # Save results\n",
    "    if data_ord == 'y_rand':\n",
    "        result_path = create_directory('./regression_results/y_rand/')\n",
    "        performance_train_df.to_csv(os.path.join(result_path, f'performance_train_y_rand.csv'))\n",
    "        performance_train_df.to_csv(os.path.join(result_path, f'performance_train_gcn_y_rand.csv'))\n",
    "        performance_test_df.to_csv(os.path.join(result_path, f'performance_test_gcn_y_rand.csv'))\n",
    "        parameter_df.to_csv(os.path.join(result_path, f'model_best_parameters_gcn_y_rand.csv'))\n",
    "        predictions_test_df.to_csv(os.path.join(result_path, f'predictions_test_gcn_y_rand.csv'))\n",
    "    else:\n",
    "        result_path = create_directory('./regression_results/regular/')\n",
    "        performance_train_df.to_csv(os.path.join(result_path, f'performance_train_gcn.csv'))\n",
    "        performance_test_df.to_csv(os.path.join(result_path, f'performance_test_gcn.csv'))\n",
    "        parameter_df.to_csv(os.path.join(result_path, f'model_best_parameters_gcn.csv'))\n",
    "        predictions_test_df.to_csv(os.path.join(result_path, f'predictions_test_gcn.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
