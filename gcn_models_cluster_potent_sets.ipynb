{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Utils\n",
    "from tqdm.notebook import tqdm\n",
    "from ml_utils import *\n",
    "from machine_learning_models import *\n",
    "from fingerprints import *\n",
    "import deepchem as dc\n",
    "from deepchem.models import GraphConvModel\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Load CCR results path\n",
    "ccr_path = \"./ccr_results/\"\n",
    "# Load actives dB\n",
    "db_path = './dataset/'\n",
    "#Load active db\n",
    "regression_db = pd.read_csv(os.path.join(db_path, f'chembl_30_IC50_10_tids_1000_CPDs.csv'))\n",
    "# Target Classes\n",
    "regression_tids = regression_db.chembl_tid.unique()[:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models Parameters\n",
    "### Select the desired parameters to be used by regression models\n",
    "<p>\n",
    "<li> <b>model_list</b>: ML/DL models for regression (kNN: k-neirest neighbor, SVR: Support Vector Regression, RFR: Random Forest Regression, DNN: Deep Neural Network, MR: Median regression)</li>\n",
    "</p>\n",
    "<p>\n",
    "<li> <b>compound_sets</b>: Compound sets to be generated ('Cluster set': Largest Analogue series, ' Potent set': Most potent compounds) </li>\n",
    "</p>\n",
    "<p>\n",
    "<li> <b>potent_size</b>: Potent sets size to be generated (0.1 = 10% original set) </li>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<li> <b>params_dict</b>: GCN hyperparameter grid (nb_epoch: number of epochs, learning_rate, graph_conv_layer, dense_layer_size, dropout, number of atom features) </li>\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model_list = ['GCN']\n",
    "compound_sets = ['Potent set', 'Cluster set']\n",
    "potent_size = 0.1\n",
    "params_dict = {\n",
    "    \"nb_epoch\":[100, 200],\n",
    "    \"learning_rate\":[0.01, 0.001],\n",
    "    \"n_tasks\":[1],\n",
    "    \"graph_conv_layers\":[[64, 64], [256, 256], [512, 512], [1024, 1024]],\n",
    "    \"dense_layer_size\":[64, 256, 512, 1024],\n",
    "    \"dropout\":[0.0],\n",
    "    \"mode\":[\"regression\"],\n",
    "    \"number_atom_features\":[75],}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "performance_train_df = pd.DataFrame()\n",
    "performance_test_df = pd.DataFrame()\n",
    "predictions_test_df = pd.DataFrame()\n",
    "predictions_train_df = pd.DataFrame()\n",
    "parameter_resume = []\n",
    "\n",
    "for target in tqdm(regression_tids):\n",
    "    for approach in compound_sets:\n",
    "            print(f'Training on {target} {approach}')\n",
    "\n",
    "            # Select Target Database\n",
    "            regression_db_tid = regression_db.loc[regression_db.chembl_tid == target]\n",
    "\n",
    "            if approach == 'Cluster set':\n",
    "\n",
    "                #Load CCR database\n",
    "                ccr_df = pd.read_csv(os.path.join(ccr_path, f'CCR_C30_IC50_HT_single_5_0.666_13_{target}.csv'))\n",
    "\n",
    "                #Load largest analogue series\n",
    "                ccr_df_AS = ccr_df.loc[ccr_df['Core'] == ccr_df['Core'].value_counts().index[0]].chembl_id.values\n",
    "\n",
    "                # Select Training and Test datasets\n",
    "                df_TR = regression_db_tid.loc[~regression_db_tid['chembl_cid'].isin(ccr_df_AS)]\n",
    "                df_TE = regression_db_tid.loc[regression_db_tid['chembl_cid'].isin(ccr_df_AS)]\n",
    "\n",
    "            elif approach == 'Potent set':\n",
    "\n",
    "                # Select Training and Test datasets\n",
    "                df_TE = regression_db_tid.nlargest(int(round(len(regression_db_tid.index)*potent_size, 0)), 'pPot')\n",
    "                df_TR = regression_db_tid.loc[~regression_db_tid['chembl_cid'].isin(df_TE['chembl_cid'])]\n",
    "\n",
    "            # Generate Mol object from SMILES\n",
    "            mols_tr = [Chem.MolFromSmiles(smi) for smi in df_TR.nonstereo_aromatic_smiles.tolist()]\n",
    "            mols_te = [Chem.MolFromSmiles(smi) for smi in df_TE.nonstereo_aromatic_smiles.tolist()]\n",
    "\n",
    "            # Constructing mol featurizer\n",
    "            featurizer = dc.feat.ConvMolFeaturizer()\n",
    "            mol_graphs_tr = featurizer.featurize(mols_tr)\n",
    "            mol_graphs_te = featurizer.featurize(mols_te)\n",
    "            mol_graphs = np.concatenate((mol_graphs_tr, mol_graphs_te))\n",
    "\n",
    "            # Potency values\n",
    "            potency = np.concatenate((np.array(df_TE.pPot.values), np.array(df_TR.pPot.values)))\n",
    "            # Compound Ids\n",
    "            ids = np.concatenate((np.array(df_TR.chembl_tid.values), np.array(df_TE.chembl_tid.values)))\n",
    "\n",
    "            # Constructing Deepchem Datasets\n",
    "            dataset = dc.data.NumpyDataset(X=mol_graphs, y=potency, ids=ids)\n",
    "            training_set_u = dc.data.NumpyDataset(X=mol_graphs_tr, y=np.array(df_TR.pPot.values), ids=np.array(df_TR.chembl_tid.values))\n",
    "            test_set_u = dc.data.NumpyDataset(X=mol_graphs_te, y=np.array(df_TE.pPot.values), ids=np.array(df_TE.chembl_tid.values))\n",
    "\n",
    "            # Initialize transformers\n",
    "            transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset, move_mean=False)]\n",
    "\n",
    "            #Transform data\n",
    "            for transformer in transformers:\n",
    "                training_set = transformer.transform(training_set_u)\n",
    "            for transformer in transformers:\n",
    "                test_set = transformer.transform(test_set_u)\n",
    "\n",
    "            for trial in range(1):\n",
    "\n",
    "                # Split dataset into TR and internal Validation\n",
    "                splitter = dc.splits.RandomSplitter()\n",
    "                train_set, valid_set = splitter.train_test_split(training_set, seed=trial)\n",
    "\n",
    "                print(f'Starting Trial {trial}')\n",
    "                for model in model_list:\n",
    "\n",
    "                    #Define random seed\n",
    "                    set_seeds(trial)\n",
    "\n",
    "                    #Initialize GridSearch optimizer\n",
    "                    optimizer = dc.hyper.GridHyperparamOpt(dc.models.GraphConvModel)\n",
    "\n",
    "                    # Select optimization metric (MAE)\n",
    "                    metric = dc.metrics.Metric(dc.metrics.mae_score)\n",
    "\n",
    "                    # Best GCN model, parameters and final results\n",
    "                    best_model, best_params, all_results = optimizer.hyperparam_search(params_dict=params_dict,\n",
    "                                                                                       train_dataset=train_set,\n",
    "                                                                                       valid_dataset=valid_set,\n",
    "                                                                                       metric=metric,\n",
    "                                                                                       use_max=False,\n",
    "                                                                                       output_transformers=transformers,\n",
    "                                                                                       #logdir=r'C:\\\\GCN\\\\'\n",
    "                                                                                       )\n",
    "                    # Define final GCN model\n",
    "                    def final_gcn(data, best_params):\n",
    "\n",
    "                        gcn = GraphConvModel(n_tasks=best_params[\"n_tasks\"],\n",
    "                                       graph_conv_layers=best_params[\"graph_conv_layers\"],\n",
    "                                       dropout=best_params[\"dropout\"],\n",
    "                                        mode=best_params[\"mode\"],\n",
    "                                       predictor_hidden_feats=best_params[\"dense_layer_size\"],\n",
    "                                       learning_rate=best_params[\"learning_rate\"],\n",
    "                                       )\n",
    "\n",
    "                        gcn.fit(data, nb_epoch=best_params[\"nb_epoch\"])\n",
    "\n",
    "                        return gcn\n",
    "\n",
    "                    #Best model parameters\n",
    "                    opt_parameters_dict = {'model': model,\n",
    "                                           'trial': trial,\n",
    "                                           'Target ID': target,\n",
    "                                           'Approach':approach}\n",
    "\n",
    "                    if isinstance(best_params, tuple):\n",
    "                        best_params = {\n",
    "                            \"nb_epoch\":best_params[0],\n",
    "                            \"learning_rate\":best_params[1],\n",
    "                            \"n_tasks\":best_params[2],\n",
    "                            \"graph_conv_layers\":best_params[3],\n",
    "                            \"dense_layer_size\":best_params[4],\n",
    "                            \"dropout\":best_params[5],\n",
    "                            \"mode\":best_params[6],\n",
    "                            \"number_atom_features\":best_params[7]}\n",
    "\n",
    "                    for param, value in best_params.items():\n",
    "                        opt_parameters_dict[param] = value\n",
    "                    parameter_resume.append(opt_parameters_dict)\n",
    "\n",
    "                    # Generate final Model\n",
    "                    ml_model = final_gcn(training_set, best_params)\n",
    "\n",
    "                    # evaluate the model\n",
    "                    train_score = ml_model.evaluate(training_set, [metric], transformers)\n",
    "                    test_score = ml_model.evaluate(test_set, [metric], transformers)\n",
    "\n",
    "                    #TRAIN\n",
    "                    #Model Evaluation\n",
    "                    model_eval_train = Model_Evaluation(ml_model, training_set, training_set_u.y, transformers[0], model_id=model)\n",
    "\n",
    "                    #Performance df\n",
    "                    performance_train = model_eval_train.pred_performance\n",
    "                    performance_train[\"trial\"] = trial\n",
    "                    performance_train[\"Approach\"] = approach\n",
    "                    performance_train_df = pd.concat([performance_train_df, performance_train])\n",
    "\n",
    "                    # Prediction df\n",
    "                    predictions_train = model_eval_train.predictions\n",
    "                    predictions_train[\"trial\"] = trial\n",
    "                    predictions_train[\"Approach\"] = approach\n",
    "                    predictions_train_df = pd.concat([predictions_train_df, predictions_train])\n",
    "\n",
    "                    #Model Evaluation\n",
    "                    model_eval_test = Model_Evaluation(ml_model, test_set, test_set_u.y, transformers[0], model_id=model)\n",
    "\n",
    "                    #Performance df\n",
    "                    performance_test = model_eval_test.pred_performance\n",
    "                    performance_test[\"trial\"] = trial\n",
    "                    performance_test[\"Approach\"] = approach\n",
    "                    performance_test_df = pd.concat([performance_test_df, performance_test])\n",
    "\n",
    "                    # Prediction df\n",
    "                    predictions_test = model_eval_test.predictions\n",
    "                    predictions_test[\"trial\"] = trial\n",
    "                    predictions_test[\"Approach\"] = approach\n",
    "                    predictions_test_df = pd.concat([predictions_test_df, predictions_test])\n",
    "\n",
    "                    del best_model, best_params, all_results, ml_model\n",
    "\n",
    "parameter_df = pd.DataFrame(parameter_resume)\n",
    "\n",
    "# Save Final Dataframes\n",
    "result_path = create_directory('./regression_results/cluster_potent/')\n",
    "performance_train_df.to_csv(os.path.join(result_path, f'performance_train_gcn_cluster_potent.csv'))\n",
    "performance_test_df.to_csv(os.path.join(result_path, f'performance_test_gcn_cluster_potent.csv'))\n",
    "parameter_df.to_csv(os.path.join(result_path, f'model_best_parameters_gcn_cluster_potent.csv'))\n",
    "predictions_test_df.to_csv(os.path.join(result_path, f'predictions_test_gcn_cluster_potent.csv'))\n",
    "predictions_train_df.to_csv(os.path.join(result_path, f'predictions_train_gcn_cluster_potent.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
